{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "therapeutic-limitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded project Example_Project!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from lib.config import cfg\n",
    "\n",
    "import lib.dataset.dataset3D as dataUtils\n",
    "from lib.hybridnet.hybridnet import HybridNet\n",
    "from lib.hybridnet.efficienttrack.efficienttrack import EfficientTrack\n",
    "import lib.hybridnet.efficienttrack.darkpose as darkpose\n",
    "from lib.config.project_manager import ProjectManager\n",
    "\n",
    "project = ProjectManager()\n",
    "project.load('Example_Project')\n",
    "\n",
    "cfg = project.get_cfg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "revised-singer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "#Load datasets... currently still needed because they contain camera parameters etc\n",
    "\n",
    "training_set = dataUtils.Dataset3D(cfg, set='train')\n",
    "val_set = dataUtils.Dataset3D(cfg, set='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "serial-trunk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] loaded weights: HybridNet-d_10.pth\n"
     ]
    }
   ],
   "source": [
    "#Load HybridNet in inference mode\n",
    "\n",
    "weightsHybridNet = '/home/timo/Documents/JARVIS-HybridNet/projects/Example_Project/models/HybridNet/Run_20211123-092606/HybridNet-d_10.pth'\n",
    "\n",
    "hybridNet = HybridNet('inference', cfg, weightsHybridNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "featured-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load center detection network in inference mode\n",
    "\n",
    "weightsCenterDetect = 'pretrained/Monkey_Example/EfficientTrack_Center.pth'\n",
    "centerDetect = EfficientTrack('CenterDetectInference', cfg, weightsCenterDetect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77979450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing Frame  0\n",
      "7\n",
      "tensor([   2.6926, -117.4809,  600.5367], dtype=torch.float64)\n",
      "7\n",
      "tensor([   2.6843, -117.4601,  600.5233], dtype=torch.float64)\n",
      "7\n",
      "tensor([   2.6292, -117.4449,  600.5494], dtype=torch.float64)\n",
      "8\n",
      "tensor([   2.6630, -117.4873,  600.5588], dtype=torch.float64)\n",
      "8\n",
      "tensor([   2.6584, -117.4054,  600.7262], dtype=torch.float64)\n",
      "8\n",
      "tensor([   2.7379, -117.3752,  600.8019], dtype=torch.float64)\n",
      "8\n",
      "tensor([   2.7337, -117.3791,  600.7705], dtype=torch.float64)\n",
      "8\n",
      "tensor([   2.7154, -117.4401,  600.5653], dtype=torch.float64)\n",
      "8\n",
      "tensor([   2.6815, -117.4467,  600.5718], dtype=torch.float64)\n",
      "8\n",
      "tensor([   2.7273, -117.4668,  600.6185], dtype=torch.float64)\n",
      "8\n",
      "tensor([   2.8187, -117.4757,  600.6901], dtype=torch.float64)\n",
      "8\n",
      "tensor([   2.7787, -117.5547,  600.6810], dtype=torch.float64)\n",
      "8\n",
      "tensor([   2.7738, -117.5415,  600.7003], dtype=torch.float64)\n",
      "8\n",
      "tensor([   2.7672, -117.5732,  600.6386], dtype=torch.float64)\n",
      "8\n",
      "tensor([   2.8370, -117.5338,  600.5540], dtype=torch.float64)\n",
      "8\n",
      "tensor([   2.8762, -117.4727,  600.5493], dtype=torch.float64)\n",
      "8\n",
      "tensor([   2.8836, -117.4821,  600.5351], dtype=torch.float64)\n",
      "8\n",
      "tensor([   2.9802, -117.4558,  600.5973], dtype=torch.float64)\n",
      "7\n",
      "tensor([   0.9776, -113.8013,  602.7697], dtype=torch.float64)\n",
      "7\n",
      "tensor([   1.0701, -113.8270,  602.8228], dtype=torch.float64)\n",
      "6\n",
      "tensor([   1.1138, -113.8703,  603.0472], dtype=torch.float64)\n",
      "5\n",
      "tensor([  -4.8025, -112.2297,  610.8328], dtype=torch.float64)\n",
      "7\n",
      "tensor([  -4.5397, -112.1541,  611.0657], dtype=torch.float64)\n",
      "7\n",
      "tensor([  -4.3938, -112.2190,  611.1381], dtype=torch.float64)\n",
      "7\n",
      "tensor([  -4.4570, -112.4492,  610.9158], dtype=torch.float64)\n",
      "7\n",
      "tensor([  -4.3185, -112.7235,  610.6652], dtype=torch.float64)\n",
      "6\n",
      "tensor([  -3.8145, -222.1092,  478.0784], dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_60567/497891936.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mreproTool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreproTool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mcameraMatrices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcameraMatrices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m hybridNet.predictPosesVideos(centerDetect, reproTool, cameraMatrices,video_paths, 'PosePredictions', \n\u001b[0m\u001b[1;32m     21\u001b[0m                              frameStart =15, frameEnd = 10000)\n",
      "\u001b[0;32m~/Documents/JARVIS-HybridNet/lib/hybridnet/hybridnet.py\u001b[0m in \u001b[0;36mpredictPosesVideos\u001b[0;34m(self, centerDetect, reproTool, cameraMatrices, video_paths, output_dir, frameStart, frameEnd, makeVideos)\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                 \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m                 \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATASET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNUM_CAMERAS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKEYPOINTDETECT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBOUNDING_BOX_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKEYPOINTDETECT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBOUNDING_BOX_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                 \u001b[0mcenterHMs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreproPoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "#Predict poses for recording and create labeled videos\n",
    "\n",
    "video_paths = [\n",
    "    'Example_Recording/Camera_B.mp4',\n",
    "    'Example_Recording/Camera_LBB.mp4',\n",
    "    'Example_Recording/Camera_LBT.mp4',\n",
    "    'Example_Recording/Camera_LC.mp4',\n",
    "    'Example_Recording/Camera_LFB.mp4',\n",
    "    'Example_Recording/Camera_LFT.mp4',\n",
    "    'Example_Recording/Camera_RBB.mp4',\n",
    "    'Example_Recording/Camera_RBT.mp4',\n",
    "    'Example_Recording/Camera_RC.mp4',\n",
    "    'Example_Recording/Camera_RFB.mp4',\n",
    "    'Example_Recording/Camera_RFT.mp4',\n",
    "    'Example_Recording/Camera_T.mp4', \n",
    "]\n",
    "\n",
    "reproTool = val_set.reproTool\n",
    "cameraMatrices = val_set.cameraMatrices\n",
    "hybridNet.predictPosesVideos(centerDetect, reproTool, cameraMatrices,video_paths, 'PosePredictions', \n",
    "                             frameStart =15, frameEnd = 10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
